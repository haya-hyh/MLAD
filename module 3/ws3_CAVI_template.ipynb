{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.0 KL Divergence\n",
    "The general idea of Variational inference is to approximate an intractable distribution with a simpler one. To achieve this, some metric is used to measure the difference between the two distributions, and some procedure to minimize this difference is applied.\n",
    "\n",
    "The most common metric used is the Kullback-Leibler divergence (KL-divergence) and is defined as:\n",
    "$$ KL(q||p) = \\int q(x) \\log \\frac{q(x)}{p(x)} dx $$  \n",
    "  \n",
    "Here, we examine the KL-divergence between different distributions and see that it makes sense with our intuition of which distributions should be similar compared to the Euclidean norm."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.0.1 KL-divergence and Euclidean norm functions and tests"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.integrate as sp_int\n",
    "\n",
    "\n",
    "def KL_divergence(p_1, p_2, a=-np.inf, b=np.inf):\n",
    "    D_KL, err = sp_int.quad(lambda x: p_1(x) * np.log(p_1(x)/p_2(x)), a, b)\n",
    "    return D_KL\n",
    "\n",
    "def Euclidean_norm(p_1, p_2, a, b):\n",
    "    D_Euc, err = np.sqrt(sp_int.quad(lambda x: (p_1(x) - p_2(x))**2, a, b))\n",
    "    return D_Euc\n",
    "\n",
    "\n",
    "# Test our functions - e.g. when the two distributions are the same distances should be 0\n",
    "mu_1_test = 0\n",
    "mu_2_test = 0\n",
    "sigma_1_test = 1\n",
    "sigma_2_test = 1\n",
    "\n",
    "p_1_test = lambda x: 1/(np.sqrt(2*np.pi) * sigma_1_test) * np.exp(-(x-mu_1_test)**2/(2 * sigma_1_test))\n",
    "p_2_test = lambda x: 1/(np.sqrt(2*np.pi) * sigma_2_test) * np.exp(-(x-mu_2_test)**2/(2 * sigma_2_test))\n",
    "\n",
    "D_KL = KL_divergence(p_1_test, p_2_test, -5, 5)  # Evaluate on [-5, 5] instead of [-inf, inf] for stability reasons\n",
    "D_Euclidean = Euclidean_norm(p_1_test, p_2_test, -5, 5)\n",
    "\n",
    "print(f\"KL-divergence: {D_KL}\")\n",
    "print(f\"Euclidean norm: {D_Euclidean}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.0.2 Plot the two distributions and compare KL-divergence and Euclidean norm\n",
    "\n",
    "Given two Gamma distributions, we plot them and compare the KL-divergence and Euclidean norm between them."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.special as sp_spec\n",
    "\n",
    "def pdf_gamma(alpha, beta):\n",
    "    # Pdfs for Gamma distributions (check Wikipedia)\n",
    "    return lambda x: beta**alpha / (sp_spec.gamma(alpha)) * x**(alpha - 1) * np.exp(-beta * x)\n",
    "\n",
    "def plot_two_pdfs(p_1, p_2, a, b):\n",
    "    x = np.linspace(a, b, 100)\n",
    "    y1 = p_1(x)\n",
    "    y2 = p_2(x)\n",
    "\n",
    "    plt.plot(x, y1, label=r'$p_1(x)$')\n",
    "    plt.plot(x, y2, label=r'$p_2(x)$')\n",
    "    plt.legend(loc='best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Case 1\n",
    "alpha_1 = 2\n",
    "beta_1 = 4\n",
    "alpha_2 = 2\n",
    "beta_2 = 3\n",
    "\n",
    "\n",
    "p_1 = pdf_gamma(alpha_1, beta_1)\n",
    "p_2 = pdf_gamma(alpha_2, beta_2)\n",
    "\n",
    "interval_a = 0\n",
    "interval_b = 3\n",
    "plot_two_pdfs(p_1, p_2, interval_a, interval_b)\n",
    "plt.show()\n",
    "\n",
    "D_KL_case1 = KL_divergence(p_1, p_2, interval_a, interval_b)\n",
    "D_Euclidean_case1 = Euclidean_norm(p_1, p_2, interval_a, interval_b)\n",
    "\n",
    "print(f\"KL-divergence: {D_KL_case1}\")\n",
    "print(f\"Euclidean norm: {D_Euclidean_case1}\")\n",
    "\n",
    "# Case 2\n",
    "alpha_1 = 100\n",
    "beta_1 = 100\n",
    "alpha_2 = 100\n",
    "beta_2 = 98\n",
    "\n",
    "p_1 = pdf_gamma(alpha_1, beta_1)\n",
    "p_2 = pdf_gamma(alpha_2, beta_2)\n",
    "\n",
    "plot_two_pdfs(p_1, p_2, interval_a, interval_b)\n",
    "plt.show()\n",
    "\n",
    "D_KL_case2 = KL_divergence(p_1, p_2, interval_a, interval_b)\n",
    "D_Euclidean_case2 = Euclidean_norm(p_1, p_2, interval_a, interval_b)\n",
    "\n",
    "print(f\"KL-divergence: {D_KL_case2}\")\n",
    "print(f\"Euclidean norm: {D_Euclidean_case2}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "relative_KL = D_KL_case1/D_KL_case2\n",
    "relative_Euclidean = D_Euclidean_case1/D_Euclidean_case2\n",
    "\n",
    "print(f\"Relative KL-divergence: {relative_KL}\")\n",
    "print(f\"Relative Euclidean norm: {relative_Euclidean}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.0.3 Discussion\n",
    "\n",
    "The Euclidean norm suggests that the differences between the distributions in the two cases are similar, which is not in line with our intuition as the case 2 distributions are more similar.\n",
    "\n",
    "The KL-divergence suggest an almost 4 times larger difference between the distributions in case 1 compared to case 2, which better captures our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 Gaussian Mixture Model (GMM)\n",
    "\n",
    "This workshops revolves around implementing the CAVI algorithm for a Gaussian Mixture Model (GMM), similar to the model which was introduced in the video lectures of Module 3, but with the simplification that $\\mu_k$ is Normal distributed and $\\tau_k$ is Gamma distributed, instead of $\\mu_k, \\tau_k$ being jointly Normal-Gamma distributed.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.1.2 Priors\n",
    "\n",
    "The priors for the GMM model are.\n",
    "\n",
    "Component means prior:\n",
    "$$ p(\\mu_k) = \\mathcal{N}(\\mu_k | \\nu_0, \\lambda_0) $$\n",
    "Component precisions prior:\n",
    "$$ p(\\tau_k) = \\text{Gamma}(\\tau_k | \\alpha_0, \\beta_0) $$\n",
    "\n",
    "Class variable prior:\n",
    "$$ p(Z | \\pi) = \\prod_{n=1}^N \\text{Categorical}(\\pi) $$\n",
    "\n",
    "Mixture weights prior:\n",
    "$$ p(\\pi) = \\text{Dir}(\\pi | \\delta_0) $$\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1.1 Model description and Bayesian Network\n",
    "\n",
    "Always start by defining the model and the Bayesian Network to get an overview of the problem.\n",
    "\n",
    "### 3.1.1.1 Observation model\n",
    "The likelihood for the GMM model is defined as:\n",
    "$$ p(X_n=x_n | \\mu, \\tau) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_k, \\tau_k) \\qquad (1)$$\n",
    "\n",
    "Instead, we will work with the model on latent variable form as (as described in Bishop 9.2):\n",
    "$$ p(X_n=x_n | Z_n, \\mu, \\tau) = \\prod_{k=1}^K \\mathcal{N}(x_n | \\mu_k, \\tau_k)^{I(Z_n = k)} \\qquad (2)$$\n",
    "\n",
    "which is an easier form to work with. $(1)$ and $(2)$ are equivalent when $(2)$ is marginalized over $Z_n$, i.e., \n",
    "$p(X_n=x_n | \\mu, \\tau) = \\sum_k p(X_n=x_n | Z_n=k, \\mu_k, \\tau_k)p(Z_n=k | \\pi)$."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.1.3 Bayesian Network\n",
    "\n",
    "Write out the Bayesian Network for the model described above.\n",
    "\n",
    "Draw on board."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1.2 Generate data\n",
    "Next we generate some data to work with. This is also a good way to get a better understanding of the model and the problem.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_data(N, K, nu_0, lambda_0, alpha_0, beta_0, delta_0):\n",
    "    return X, Z_true, pi_true, mu_true, tau_true\n",
    "\n",
    "def generate_pi(delta_0):\n",
    "    pass\n",
    "\n",
    "def generate_mu(nu_0, lambda_0, K):\n",
    "    pass\n",
    "\n",
    "def generate_tau(a_0, b_0, K):\n",
    "    pass\n",
    "\n",
    "def generate_Z(N, pi_true):\n",
    "    pass\n",
    "\n",
    "def generate_X(N, mu_true, tau_true, Z):\n",
    "    pass\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test the data generation functions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "K_test = 2\n",
    "N_test = 100\n",
    "\n",
    "# Fix mu_1 and mu_2 and tau_1 and tau_2\n",
    "mu_1_test = 100\n",
    "mu_2_test = 1\n",
    "mu_test = [mu_1_test, mu_2_test]\n",
    "tau_test = [1, 1]\n",
    "\n",
    "# Fix pi - biased towards component 1\n",
    "pi_test = np.array([0.8, 0.2])\n",
    "\n",
    "Z_test = generate_Z(N_test, pi_test)\n",
    "X_test = generate_X(N_test, mu_test, tau_test, Z_test)\n",
    "\n",
    "print(f\"X mean: {X_test.mean()}\")\n",
    "assert X_test.mean() > 60.0  # Expect the mean of X to be much closer to mu_1 than mu_2\n",
    "Z_test_one_hot = np.eye(K_test)[Z_test]\n",
    "print(f\"Z: {Z_test_one_hot.mean(axis=0)}\")\n",
    "assert np.allclose(Z_test_one_hot.mean(axis=0), pi_test, atol=0.1)  # Expect the mean of Z to be close to pi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1.3 Coordinate ascent VI (CAVI)\n",
    "\n",
    "General algorithm for CAVI for the GMM:\n",
    "1. Initialize the variational parameters $\\gamma$.\n",
    "2. While the ELBO has not converged:\n",
    "    3. Update the variational parameters for each latent variable $z_n$.\n",
    "    4. Update the variational parameters for each component mean $\\mu_k$.\n",
    "    5. Update the variational parameters for each component precision $\\tau_k$.\n",
    "    6. Update the variational parameters for the class variable $\\pi$.\n",
    "    7. Calculate the ELBO. (To measure convergence).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CAVI updates\n",
    "\n",
    "Derivation on board, and taken from the video lectures.\n",
    "\n",
    "When deriving the CAVI updates, always begin with writing out the factorization of the joint:\n",
    "\n",
    "$$ p(X, Z, \\mu, \\tau, \\pi) = p(X | Z, \\mu, \\tau) p(Z | \\pi) p(\\mu) p(\\tau) p(\\pi) $$\n",
    "\n",
    "And the minimum mean-field approximation:\n",
    "$$ q(Z, \\mu, \\tau, \\pi) = q(Z)q(\\pi, \\mu, \\tau)$$\n",
    "\n",
    "Which, in video lectures, are shown to simplify further to: \n",
    "$$q(Z)q(\\pi, \\mu, \\tau) = q(Z) q(\\pi) \\prod_k q(\\mu_k) q(\\tau_k) $$\n",
    "\n",
    "Then apply the CAVI update equations for each variational distribution. Let's start with $q(\\mu_k)$:\n",
    "\n",
    "$$ \\log q^*(\\mu_k) \\stackrel{+}{=} \\mathbb{E}_{q(Z, \\mu_{\\neg k}, \\tau, \\pi)}\\big[\\log p(X, Z, \\mu, \\tau, \\pi)\\big] $$\n",
    "\n",
    "Derive on board."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fixed updates"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1.4 Running the CAVI algorithm on simulated data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def CAVI_update_mu_k(x, q_Z_k, E_tau_k, nu_0, lambda_0):\n",
    "    \"\"\"\n",
    "    Implement this based on the CAVI update derived on the board.\n",
    "    \"\"\"\n",
    "    return nu_star, lambda_star\n",
    "\n",
    "\n",
    "# Updates based on the video lectures (slight modifications)\n",
    "def CAVI_update_Z(E_log_tau, E_tau, E_x_mu2, E_log_pi):\n",
    "    return rho_star\n",
    "\n",
    "\n",
    "def CAVI_update_tau(q_Z, alpha_0, beta_0, E_x_mu2):\n",
    "    return alpha_star, beta_star\n",
    "\n",
    "\n",
    "def CAVI_update_pi(q_Z, delta_0):\n",
    "    return delta_star\n",
    "\n",
    "\n",
    "def calculate_ELBO(x, r_q, nu_q, lambda_q, alpha_q, beta_q, delta_q, prior_params):\n",
    "    nu_0, lambda_0, alpha_0, beta_0, delta = prior_params\n",
    "    N, K = x.shape[0], nu_q.shape[0]\n",
    "\n",
    "    E_tau = alpha_q / beta_q\n",
    "    E_log_tau = sp_spec.digamma(alpha_q) - np.log(beta_q)\n",
    "    E_log_pi = sp_spec.digamma(delta_q) - sp_spec.digamma(np.sum(delta_q))\n",
    "    E_mu = nu_q\n",
    "    E_mu2 = nu_q ** 2 + 1 / lambda_q ** 2\n",
    "\n",
    "    x_n2_rnk_sum_n = np.einsum('n, nk->k', x ** 2, r_q)\n",
    "    x_n_rnk_sum_n = np.einsum('n, nk->k', x, r_q)\n",
    "    rnk_sum_n = np.einsum('nk->k', r_q)\n",
    "\n",
    "    ELBO_p_X_Z_mu_tau = 1/2 * np.einsum('k, k->', E_log_tau, np.sum(r_q, axis=0)) - N/2 * np.log(2 * np.pi) - \\\n",
    "                1/2 * np.einsum('k, k->', E_tau, x_n2_rnk_sum_n - 2 * x_n_rnk_sum_n * E_mu + E_mu2 * rnk_sum_n)\n",
    "\n",
    "    ELBO_p_Z_pi = np.einsum('nk,k->', r_q, E_log_pi)\n",
    "    ELBO_p_mu = np.einsum('k->', 1/2 * np.log(lambda_0) - 1/2 * np.log(2*np.pi)\n",
    "                          - 1/2 * lambda_0 * (E_mu2 - 2*E_mu*nu_0 + nu_0**2))\n",
    "    ELBO_p_tau = np.einsum('k->', alpha_0*np.log(beta_0) - sp_spec.gammaln(alpha_0)\n",
    "                            + (alpha_0 - 1)*E_log_tau - beta_0*E_tau)\n",
    "    log_Beta_func_delta_0 = sp_spec.gammaln(delta).sum() - sp_spec.gammaln(np.sum(delta))\n",
    "    ELBO_p_pi = np.einsum('k->', (delta - 1) * E_log_pi) - log_Beta_func_delta_0\n",
    "\n",
    "    H_qZ = -np.einsum('nk->', r_q * np.log(r_q))\n",
    "    H_qmu = 1/2 * np.einsum('k->', -np.log(lambda_q) + 1 + np.log(2*np.pi))\n",
    "    H_qtau = np.einsum('k->', sp_spec.gammaln(alpha_q) - (alpha_q - 1)*sp_spec.psi(alpha_q) - np.log(beta_q)\n",
    "                        + alpha_q)\n",
    "    log_Beta_func_delta_q = sp_spec.gammaln(delta_q).sum() - sp_spec.gammaln(np.sum(delta_q))\n",
    "    delta_q_sum = np.sum(delta_q)\n",
    "    H_qpi = log_Beta_func_delta_q + (delta_q_sum - K)*sp_spec.digamma(delta_q_sum) - ((delta_q - 1)*sp_spec.digamma(delta_q)).sum()\n",
    "\n",
    "    ELBO = ELBO_p_X_Z_mu_tau + ELBO_p_Z_pi + ELBO_p_mu + ELBO_p_tau + ELBO_p_pi + H_qZ + H_qmu + H_qtau + H_qpi\n",
    "    return ELBO\n",
    "\n",
    "\n",
    "def CAVI_algorithm(x, K, prior_params, max_iter=100, tol=1e-3, step_size=0.01):\n",
    "    N = x.shape[0]\n",
    "    nu_0, lambda_0, alpha_0, beta_0, delta_0 = prior_params\n",
    "\n",
    "    # Define Variational parameters\n",
    "    I = max_iter + 1\n",
    "    r_q = np.zeros((I, N, K))                               # N x K (params for q(Z))\n",
    "    alpha_q, beta_q = np.zeros((I, K)), np.zeros((I, K))    # K     (params for q(tau))\n",
    "    nu_q, lambda_q = np.zeros((I, K)), np.zeros((I, K))     # K     (params for q(mu))\n",
    "    delta_q = np.zeros((I, K))                              # K     (params for q(pi))\n",
    "\n",
    "    # Define Expected Values functions\n",
    "    E_tau_map = lambda alpha, beta: alpha / beta\n",
    "    E_log_tau_map = lambda alpha, beta: sp_spec.psi(alpha) - np.log(beta)\n",
    "    E_log_pi_map = lambda delta: sp_spec.psi(delta) - sp_spec.psi(np.sum(delta))\n",
    "    E_mu_map = lambda nu: nu\n",
    "    E_mu2_map = lambda nu, lmbda: nu ** 2 + 1 / lmbda ** 2\n",
    "    E_x_mu2_map = lambda x, nu, lmbda: ((np.einsum('n,n,k->nk', x, x, np.ones(K))\n",
    "                                        - 2 * np.einsum('n, k -> nk', x, E_mu_map(nu)))\n",
    "                                        + np.einsum('k,n->nk', E_mu2_map(nu, lmbda), np.ones(N)))\n",
    "\n",
    "    # Initialize the variational parameters\n",
    "    labels = sklearn.cluster.KMeans(n_clusters=K).fit(x.reshape(-1, 1)).labels_\n",
    "    delta_q[0] = delta_0\n",
    "    alpha_q[0], beta_q[0] = alpha_0, beta_0\n",
    "    lambda_q[0] = lambda_0\n",
    "    for k in range(K):\n",
    "        x_k = x[labels == k]\n",
    "        nu_q[0, k] = np.mean(x_k)\n",
    "\n",
    "    # Initialize expected values\n",
    "    E_tau = E_tau_map(alpha_q[0], beta_q[0])\n",
    "    E_log_tau = E_log_tau_map(alpha_q[0], beta_q[0])\n",
    "    E_log_pi = E_log_pi_map(delta_q[0])\n",
    "    E_x_mu2 = E_x_mu2_map(x, nu_q[0], lambda_q[0])  # N x K\n",
    "\n",
    "    ELBO = np.zeros(max_iter)\n",
    "    \n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \"\"\"\n",
    "        Implement the for loop of the CAVI algorithm here.\n",
    "        \"\"\"\n",
    "\n",
    "    out = {'ELBO': ELBO, 'r_q': r_q, 'nu_q': nu_q, 'lambda_q': lambda_q,\n",
    "           'alpha_q': alpha_q, 'beta_q': beta_q, 'delta_q': delta_q}\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N_sim = 1000\n",
    "K_sim = 3\n",
    "mu_true = np.array([1, 3, 6])\n",
    "tau_true = np.array([5, 5, 5])\n",
    "pi_true = np.array([0.2, 0.3, 0.5])\n",
    "Z_true = generate_Z(N_sim, pi_true)\n",
    "\n",
    "x = generate_X(N_sim, mu_true, tau_true, Z_true)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Histogram of the data\n",
    "plt.hist(x, bins=30)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run CAVI"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Algorithm parameters\n",
    "np.random.seed(0)\n",
    "max_iter = 1000\n",
    "tol = 1e-3\n",
    "K = 3\n",
    "N = x.shape[0]\n",
    "step_size = 0.1\n",
    "\n",
    "# Prior parameters\n",
    "nu_0 = np.array([1., 1., 1.]) * 2.\n",
    "lambda_0 = 10.\n",
    "alpha_0 = 1.\n",
    "beta_0 = 1.\n",
    "delta_0 = np.ones(K) * N/K\n",
    "prior_params = (nu_0, lambda_0, alpha_0, beta_0, delta_0)\n",
    "\n",
    "# Run CAVI\n",
    "out = CAVI_algorithm(x, K, prior_params, max_iter, tol, step_size=step_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot the ELBO"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(out['ELBO'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ELBO')\n",
    "plt.title('ELBO over iterations')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize the model fit"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.stats as sp_stats\n",
    "x_axis = np.linspace(-2, 15, 1000)\n",
    "plt.hist(x, bins=30, density=True, alpha=0.5, label='Data')\n",
    "for k in range(K):\n",
    "    mu_MAP_k = out['nu_q'][-1][k]\n",
    "    tau_MAP_k = out['alpha_q'][-1][k] / out['beta_q'][-1][k]\n",
    "    pi_MAP_k = out['delta_q'][-1][k] / np.sum(out['delta_q'][-1])\n",
    "    plt.plot(x_axis, pi_MAP_k * sp_stats.norm.pdf(x_axis, mu_MAP_k, 1 / np.sqrt(tau_MAP_k)),\n",
    "             label=f'Component {k + 1}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot parameters trajectories"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(out['alpha_q'], out['beta_q'])\n",
    "plt.xlabel('alpha_q')\n",
    "plt.ylabel('beta_q')\n",
    "plt.title('alpha_q and beta_q trajectories')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Label switching\n",
    "\n",
    "The labels of the clusters inferred by the algorithm can be permuted compared to the true labels. This is known as the label switching problem. One way to handle this is to find the permutation of the inferred labels that best matches the true labels."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"True pi: {pi_true}\")\n",
    "print(f\"E_q[pi]: {out['delta_q'][-1]/np.sum(out['delta_q'][-1])}\")\n",
    "print(f\"True mu: {mu_true}\")\n",
    "print(f\"E_q[mu]: {out['nu_q'][-1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
